---
title: "14 Linear Mixed Effects Models (LME) "
author: "Kristopher Kyle"
date: "3/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear mixed effects models


### Description of sample data

This data comprises L2 English essays written over a two year period by nine middle-school aged Dutch children studying at an English/Dutch bilingual school in the Netherlands. Essays were collected three times a year (roughly every four months) over two academic years. Included in the dataset are holistic scores for each essay ("Score") and mean length of T-unit (MLT) values. In this tutorial, we will explore the relationship between MLT and time spent studying English, with the alternative hypothesis that MLT scores will increase as a function of time. For further reference, see Kyle (2016).

```{r}
mydata <- read.csv("data/RM_sample.csv", header = TRUE)
#First, we create a new variable that is the categorical version of Time
mydata$FTime <- factor(mydata$Time)
summary(mydata)
```

### Visualizing the data

First, we can look at the means at each time point.

```{r}
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Time)) +
  geom_boxplot()
```

Then, we can get a clearer view by looking at individual trajectories.

```{r}
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
  geom_line(aes(color=Participant)) +
  geom_point(aes(color=Participant))
```
Sometimes, we get a clearer view of individual trajectories when we use facet wrap:

```{r}
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
  geom_line(aes(color=Participant)) +
  geom_point(aes(color=Participant)) +
  facet_wrap(~Participant)
```

### Conducting an LME

#### Random intercepts

In this case, our random effects are our participants (i.e., our main interest is not in intra-participant variation, but we still need to control for it). In the first model we will run, we will presume that all of our participants will follow similar trajectories, but may have different starting points (intercepts). In other words, the model will presume that participants will increase their MLT scores at approximately the same rate, but may have be at different writing proficiencies (at least with regard to MLT). Also, first, we will run the analysis with time as a categorical variable (FTime), like we would with a repeated measures ANOVA. Then, we will run it as a continuous variable (FTime) - which it is.

```{r}
library(lmerTest)
int_model1 <- lmer(MLT~FTime + (1 | Participant), data=mydata)
summary(int_model1)
```

Note that the "intercept" here presumes that FTime = 1, and all of the data points are in reference to this. So, from FTime 1 to FTime 2, there is a .6667 increase (which is not significant). Additionally, from FTime 1 to FTime 3, there is a 1.7767 increase (which is not significant). We don't get a significant change until we get to FTime 5. 

We can also see the degree to which our estimates need to change for each participant (i.e., we can see our individualized random effects).

```{r}
ranef(int_model1)
```

Again, this is a random intercepts model, so the only changes we will see is in the intercept. All other estimates will be in relation to these revised intercepts. So, for example, at Time 1, EFL_1's estimated MLT score will be 9.6968 - 0.3623567 = 9.334443, while EFL_2's estimated MLT score will be 9.6968 + 1.2929820 = 10.98978. But remember, these are estimates created under the assumption that all participants will have the same slope (or rate of change), and we can see from looking at our plots that this estimate is decent for EFL_1, but rather poor for EFL_2.

The mismatch between estimated and actual scores is reflected in the effect size of the model, which we can obtain using the "MuMIn" package.

```{r}
library(MuMIn)
r.squaredGLMM(int_model1)
```

The "MuMIN" package gives us two R^2 values. The first, R2m (marginal r-squared), is the effect size without considering the random effects (i.e., where the model is built without regard to participant differences). The second, R^2c (conditional r-squared) is the effect size when the random effects are taken into account. Accordingly, the R^2c values are aways higher than the R^2m values (unless there are little/no random effects). 
Below, we use the package "emmeans" to get pairwise comparisons from our model (in case we want them). The emmeans packages is very flexible, so it is probably worthwhile to read the documentation on the package.

```{r}
library(emmeans)
emmeans(int_model1, specs = pairwise~FTime)
```

We can also run our analysis with Time as a continuous variable (the distance between each Time value is roughly 4-months). In this case, we will be testing whether the line of best fit is a reasonable approximation of our data. Again, at this point, we are still working with a random intercepts model.

```{r}
int_model2 <- lmer(MLT~Time + (1 | Participant), data=mydata)
summary(int_model2) #get model summary
ranef(int_model2) #get slope adjustments per participant
r.squaredGLMM(int_model2) #get effect sizes
```

This indicates that Time is a significant predictor of MLT scores (p = .00027) with medium effects (R2^m = 0.191, R^2c = 0.355). Note below, we add the predicted scores for each participant to our dataframe using the "mutate" function of the package "dplyr". Then, we plot the predicted line (dashed) on top of the actual data. We see that each line has the same slope, but different intercepts.

```{r}
library(dplyr)
#add predicted values to dataframe
mydata <- mydata %>% mutate(int_model2_pred = predict(int_model2,re.form = ~ (1 | Participant)))

ggplot(mydata, aes(y=MLT, x=Time)) +
  facet_wrap(~ Participant) + 
  geom_point(aes(color=Participant), show.legend = FALSE) + 
  geom_line(aes(color=Participant), show.legend = FALSE) +
  geom_line(aes(y=int_model2_pred), linetype=2) +
  scale_y_continuous()
```

#### Random intercepts and random slopes

Above, we fit linear models with random intercepts, but fixed slopes. LMEs are quite flexible, however, and also allow us to fit more complicated models. Below, we will run a model with random intercepts AND random slopes, and then evaluate whether the more complicated model is reasonably better than the simpler one. We will almost always get a more accurate model when we add complexity, but the comlexity may not always be justified (e.g., it is a lot more difficult to explain a random intercepts and slopes model, and we only want to do so if it significantly increases our explanatory power). Note that we could treat Time either as a categorical variable or as a continuos variable. However, below we will only look at the continuous model.

```{r}
int_slope_model <- lmer(MLT~Time + (1 + Time | Participant), data=mydata)
summary(int_slope_model) #get model summary
```

As we see above, our results look almost identical to the previous model (int_model_2). The only difference is that we have statistics for one more random effect (Time) - our slope. Below, we get by participant effects, which now include adjustments for slope in addition to those for intercept. We also see that our R^2m is almost identical, while our R^2c has increased slightly.

```{r}
ranef(int_slope_model)
r.squaredGLMM(int_slope_model)
```

When we plot our actual and predicted values, we now see that each participant has a different intercept and a slightly different slope. However, in this case, the differences are not extreme.

```{r}
library(dplyr)
mydata <- mydata %>% mutate(int_slope_model_pred = predict(int_slope_model,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
  facet_wrap(~ Participant) + 
  geom_point(aes(color=Participant), show.legend = FALSE) + 
  geom_line(aes(color=Participant), show.legend = FALSE) +
  geom_line(aes(y=int_slope_model_pred), linetype=2) +
  scale_y_continuous()
```

To determine whether the more complicated model (random intercepts + slopes) is significantly better at modeling our data than the simpler one (random intercepts), we simply use the "anova" function, which indicates that there are indeed no significant differences between our models (p = .6093), so we might as well use the simpler one.

```{r}
anova(int_model2,int_slope_model)
```


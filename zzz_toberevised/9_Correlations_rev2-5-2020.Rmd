---
title: "Correlation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Correlation: Examining the relationship between variables

### The basics
A correlation is a measure that indicates the degree to which the values of one variable change with respect to another variable. For example, we might examine the relationship between the temperature and the number of students wearing a sweatshirt/jacket on campus. In second language learning, we might look at the relationship between the number of hours a student spends studying and their scores on a proficiency test. For most of this tutorial, we will be looking at a common type of correlational analysis (Pearson's product-moment correlation), though the calculation of other correlation coefficients will be briefly discussed at the end.

Importantly, correlation values (hereafter referred to as _r_) are an effect size that range from -1.0 to 1.0. When we interpret the size of a correlation, we are concerned with the absolute value of the number, not the directionality. For example, an _r_ value of -.700 is bigger than an _r_ value of .300. 

The sign of the _r_ value tells us whether the values from variable A increase as the values of variable B increase (resulting in a positive sign) OR if the variable A values decrease as the variable B values increase (a negative sign).

Finally, _p_ values are also calculated for correlation analyses. These values tell us the probability that observed relationship between the variables would be observed if there were actually no relationship between them.

### Assumptions
Pearson's correlation has the following assumptions:

- The variables must be continuous (see other tests for ordinal or categorical data)

- The variables must have a linear relationship

- There are no outliers (or there are only minimal outliers in large samples)

- The variables must have a bivariate normal distribution (but we will settle for each variable being roughly normally distributed)


### Checking assumptions
In our first example, we will examine the relationship between  between number of words (as a proxy for proficiency) and lexical sophistication (measured as word frequency) in a corpus of argumentative essays written as part of a standardized test of English proficiency.

These variables (and a few others) are included in the "correlation_sample.csv" file included on our Canvas page.

``` {r}
library(ggplot2) #load ggplot2
cor_data <- read.csv("data/correlation_sample.csv", header = TRUE) #read the spreadsheet "correlation_sample.csv" into r as a dataframe
summary(cor_data) #get descriptive statistics for the dataset
```

#### Assumption 1: Continuous or ratio data
Our data for each variable is continuous (it is not, for example, categorical), so we can continue with our analysis.

#### Assumption 2: Linearity
To check the linearity of our data, we will create a scatterplot. For our data to meet the criteria of linearity, it will need to fall in roughly a straight line (and not one that is curvilinear).

```{r}
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
  geom_point()
```

The data does not appear to be curvilinear, and there appears to be a negative linear relationship between number of words in an essay and the frequency of content words (nouns, verbs, adjectives, and adverbs of manner).

#### Assumption 3: Minimal outliers
Outliers can strongly affect our correlation coefficents (particularly in small datasets). Because our dataset is fairly large (480 participants), having a few outliers will not be a large problem. To check for outliers, lets take a look at the scatterplot again.

```{r}
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
  geom_point()
```

It appears as though we only have a few outliers (and one particularly extreme one in the bottom right section of the plot.) Based on the size of the data, there don't seem to be any large issues to be worried about.

#### Assumption 4: Normality
Next, we will check the assumption of normality for our variables using histograms Shapiro-Wilks tests. First, we will look at number of words (nwords) and then we will look at frequency of content words (frequency_CW)

```{r}
library(ggpubr) #load library
ggdensity(cor_data, x = "nwords",
          add = "mean", rug = TRUE)

shapiro.test(cor_data$nwords)
```
```{r}
ggdensity(cor_data, x = "frequency_CW",
          add = "mean", rug = TRUE)

shapiro.test(cor_data$frequency_CW)
```

The histograms suggests that the distribution of each index is roughly normal. However, a Shapiro-Wilks tests (see below) indicate that both distributions deviates from a normal distribution.

In a real study, we would have to decide how grossly we violate normality and what effects that might have on our generalizations. For the purposes of this tutorial, we will decide that the violation isn't extreme and will use a Pearson correlation.


### Calculating Pearson's _r_ 
Now, we will actually calculate the correlation between number of words (nwords) and content word frequency (frequency_CW) using the cor.test function, which will give us an _r_ value and a _p_ value.

```{r}
cor.test(cor_data$nwords,cor_data$frequency_CW)
```

In this case, the results indicate a significant relationship between number of words (nwords) and content word frequency (frequency_CW): _p_ = 7.201e-06, which translates to .000007201. In other words, there is a .0007% chance that we would observe this data if there were no relationship between number of words and content word frequency.

The correlation value ( _r_ = -.203) indicates that a) there is an inverse relationship between number of words and content word frequency, and b) this relationship is fairly weak (Cohen, 1988 suggests that small/weak correlations range from .100 to .299).

### Other correlation tests
If your variables do not meet the assumptions of the Pearson product moment correlation, all is not lost! The following correlation coefficents can also be calculated.

#### Ordinal data: Spearman's Rho and Kendall's Tau
Also included in our dataset is the variable Score, which is ordinal (there are only eight possible scores). Although we may be able to make an argument for the treatment of this variable as continuous (and therefore eligible for use with Pearson's product moment correlation), it is safer to use Spearman's Rho or Kendall's Tau.

Below, we will plot the relationship between Score and content word frequency, and then conduct the statistical test.

```{r}
ggplot(cor_data, aes(x = Score, y=frequency_CW )) +
  geom_point()
```

```{r}
cor.test(cor_data$Score,cor_data$frequency_CW,method = ("spearman"))
cor.test(cor_data$Score,cor_data$frequency_CW,method = ("kendall"))
```

The results from both tests indicate a significant relationship. Further, both tests indicate that the relationship between Score and content word frequency is negative. The Spearman Rho value (-.299) indicates a small/weak effect that is approching a medium effect. The Kendall Tau value (-.219) is more conservative and indicates a small/weak effect.

#### Categorical variables: Biserial correlation
If we want to look at the relationship between a categorical variable and a continuous variable, we can use biserial correlations. In our dataset, we have one categorical variable (pass.fail) which indicates whether the essay score is considered a "pass" (score of 3 or better) or a "fail" (score below 3). We will look at the relationship between pass.fail and number of words (nwords). First, we will plot the data, then we will the biserial correlation.

```{r}
ggplot(cor_data, aes(x = nwords, y=pass.fail )) +
  geom_point()
```

The plots suggest a linear relationship, though it appears as though this relationship is likely weak due to the overlapping values. Below, we will run the tests.

```{r}
library(psych) #you may need to install this package
#note that the dichotomous categorical variable must be the second (y) variable!
biserial(cor_data$nwords,cor_data$pass.fail)
```

The results indicate that the biserial correlation between number of words and pass/fail (.666) is positive and large (according to Cohen, 1988).

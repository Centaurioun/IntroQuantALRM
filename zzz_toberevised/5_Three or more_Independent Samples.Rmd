---
title: "One Way ANOVA"
author: "Kristopher Kyle"
date: "3/22/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Examining differences between more than two groups
When we want to examine differences between two independent groups, a t-test or Man-Whitney U test are the most appropriate to use. If we have more than two independent groups, however, it is not appropriate to use these tests. Instead, we should use an Analysis of Variance (ANOVA) test or one of the non-parametric alternatives.

### Assumptions of One-Way ANOVA
To use an ANOVA, our data must meet the following assunptions:

1. The groups are independent (and do not include repeated measures)
2. There is a normal distribution of values in each group
3. The variance in each group is equal (this is call homogeneity of variance)

Note that we will be double checking some of these assumptions after conducting the analysis due to the way that particular R packages have been written.

### Description of sample data
In this tutorial, we will be looking at essays written by individuals at three different levels of lexical proficiency (Beginner, Intermediate, and High). We will also be looking at the average meaningfulness score for all words in each text. Meaningfulness scores are one way of determining how many concepts a particular word might be related to (e.g., the word "food" would be more "meaningful" than "zygote" because it could be used in more semantic contexts). We will be determining whether there are any differences in average meaningfulness scores based on lexical proficiency group.

```{r}
mydata <- read.csv("data/anova_sample.csv", header = TRUE)
summary(mydata)
```
### Visualizing the data
First, we can create density plots to look at the distribution of each group. These density plots suggest that the data is roughly normal and that the variance is roughly equal.

```{r}
library("ggpubr")
ggdensity(mydata, x = "Meaningfulness_AW",
          add = "mean", rug = TRUE,
          color = "Proficiency", fill = "Proficiency")
```

Second, we will generate some boxplots. These boxplots support the notion that the variance is roughly equal, and also suggests that there may be meaningful differences between the groups. 

```{r}
ggboxplot(mydata, x = "Proficiency", y = "Meaningfulness_AW", 
          color = "Proficiency", 
          order = c("Beginner", "Int", "High"),
          ylab = "Meaningfulness Scores (All Words)", xlab = "Proficiency")
```

Third, we can look at mean plots, which further suggest that there may be differences between the groups.

```{r}
ggline(mydata, x = "Proficiency", y = "Meaningfulness_AW", 
       add = c("mean_se", "jitter"), 
       order = c("Beginner", "Int", "High"),
       ylab = "Meaningfulness Scores (All Words)", xlab = "Proficiency")

```

### Conducting the analysis
First, we will fit a linear regression to demonstrate that an ANOVA and a linear regression are related analyses. Then, we will conduct an ANOVA with post-hoc tests and provide some non-parametric alternatives.

### Getting descriptive statistics
First, we will get our descriptive statistics.

```{r}
library(psych)
describeBy(mydata$Meaningfulness_AW,mydata$Proficiency)
```

#### Fitting a linear regression
First, we will run a linear model (i.e., a regression) for comparison.
```{r}
regr_model <- lm(Meaningfulness_AW~Proficiency, data = mydata) #note that for this to work, our dependent variable has to be numeric
summary(regr_model)
```

These results suggest that there is a linear relationship between Proficiency and Meaninfulness scores,

#### Conducting an ANOVA
Next, we will run an ANOVA, which will tell us whether there are significant differences between any of the groups:

```{r}
library("lsr")
anova_model <- aov(Meaningfulness_AW~Proficiency, data = mydata)
summary(anova_model)
etaSquared(anova_model, type = 3)
```

Another option to the "lsr" package for running effect sizes is the "sjstats" package, which also computes omega-squared.

```{r}
library(sjstats)
eta_sq(anova_model)
omega_sq(anova_model)
```

The results indicate that there are indeed differences between our groups (p = .000000006), and that these differences are meaningful (eta squared = 0.148 -  note that this is the same as our linear regression, omega squared = 1.40), but they don't tell us where those differences are.

#### Conducting pairwise comparisons
To figure out where the differences are, we have to run pairwise comparisons. In this case, we will conduct Tukey HSD (Honest Significant Differences) test, which nicely balances the potential for Type I and Type II errors. As we see below, there are significant differences between each of our groups. Note that the observed differences relate to the linear regression model above.

```{r}
TukeyHSD(anova_model)
```

To get an effect size for each contrast, we will have to do a little more work. Below, we create a new dataframe using the "filter" and "select" functions of dplyr to select only rows that have a Proficiency value of "Beginner" or "Int" (using filter), and only the columns Proficiency and Meaningfulness_AW. We then use the function "cohen.d" in the psych package to calculat the effect size between the "Beginner" and "Int" groups. As we see below, the effect between these groups is .54, which represents a medium effect.

```{r}
library(dplyr)
#create new df with only Beginner and Int
mydata_effs_BI <- mydata %>%
  filter(Proficiency == "Beginner" | Proficiency == "Int") %>%
  select(Proficiency, Meaningfulness_AW)
summary(mydata_effs_BI)

#create new df with only Beginner and High
mydata_effs_BH <- mydata %>%
  filter(Proficiency == "Beginner" | Proficiency == "High") %>%
  select(Proficiency, Meaningfulness_AW)
summary(mydata_effs_BH)

#create new df with only Int and High
mydata_effs_IH <- mydata %>%
  filter(Proficiency == "Int" | Proficiency == "High") %>%
  select(Proficiency, Meaningfulness_AW)
summary(mydata_effs_IH)

#note, this uses the psych package:
cohen.d(mydata_effs_BI,"Proficiency")
```

To get the effect sizes for the rest of the contrasts, we repeat this process with the other group combinations ("Beginner" to "High" and "Int" to "High").

```{r}
#create new df with only Beginner and High
mydata_effs_BH <- mydata %>%
  filter(Proficiency == "Beginner" | Proficiency == "High") %>%
  select(Proficiency, Meaningfulness_AW)

#create new df with only Int and High
mydata_effs_IH <- mydata %>%
  filter(Proficiency == "Int" | Proficiency == "High") %>%
  select(Proficiency, Meaningfulness_AW)

#note, this uses the psych package:
cohen.d(mydata_effs_BH,"Proficiency")
cohen.d(mydata_effs_IH,"Proficiency")
```

### Checking assumptions
#### Homogeneity of variance
First, we can visually check the relationship between the residuals (e.g., the error) and the fitted values to ensure that there isn't a relationship between these two. As we see below, our line is flat, indicating that there is not a relationship between these.

```{r}
plot(anova_model,1)
```

Second, we can conduct Levene's test, which determines the degree to which the variance differs across groups.

```{r}
library(car)
leveneTest(Meaningfulness_AW~Proficiency, data = mydata)
```

The test for homogeneity of variance indicates that there is a 12.88% chance that we would observe the differences in variance if there were no difference. Because this chance is above 5% (p = .05), it can be suggested that the groups have equal variance.

#### What to do if we violate homogeneity of variance
If our groups are normally distributed but violate homogeneity of variance, we can compute the Welch one-way test with Games-Howell post hoc tests. Note that we would still use eta squared as our effect size, which would would generate using the code above.

```{r}
library(userfriendlyscience)
oneway(mydata$Proficiency, y = mydata$Meaningfulness_AW, posthoc = 'games-howell')
```


#### Double checking normality using QQ plots
We have already visually checked for normality using density plots. However, we can also check for normality using a Q-Q plot. In this case our data appears to be roughly normally distributed, confirming our visual inspection of the density plots.

```{r}
plot(anova_model,2)
```

#### What if my data isn't normally distributed?
If our data is NOT normally distributed, there are non-parametric alternatives. One of these is the Kruskal-Wallis rank sum test:

```{r}
kruskal.test(Meaningfulness_AW~Proficiency, data = mydata)
```

One option for pairwise comparisons with the Kruskal- Wallis test is the Dunn test:

```{r}
library(FSA)
dunnTest(Meaningfulness_AW~Proficiency, data = mydata, method = "bh")
```


